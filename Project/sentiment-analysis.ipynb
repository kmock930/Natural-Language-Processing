{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d3c453d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0f59d",
   "metadata": {},
   "source": [
    "Research Question: What linguistic patterns and sentiment markers are most indicative of suicidal intent in social media text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad780e9",
   "metadata": {},
   "source": [
    "Note: This notebook exports analysis results to `.jsonl` files under `N-Gram Analysis` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e6433",
   "metadata": {},
   "source": [
    "## Import Relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb313e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation of nltk.sentiment.vader success?: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kelvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "DATA_PATH = os.path.abspath(os.path.join(os.path.dirname(os.curdir), 'data'))\n",
    "ASSIGNMENT_1_PATH = os.path.abspath(\n",
    "    os.path.join(\n",
    "        os.path.dirname(os.curdir), \n",
    "        \"..\",\n",
    "        'Assignment 1',\n",
    "        \"src\"\n",
    "        )\n",
    ")\n",
    "sys.path.append(DATA_PATH)\n",
    "sys.path.append(ASSIGNMENT_1_PATH)\n",
    "import zipfile\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "print(f\"Installation of nltk.sentiment.vader success?: {'Yes' if nltk.download('vader_lexicon') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "810c9b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stopwords in consideration: 779\n"
     ]
    }
   ],
   "source": [
    "stopwords = open(\n",
    "    os.path.join(\n",
    "        ASSIGNMENT_1_PATH, \n",
    "        \"StopWords.txt\"\n",
    "    ), \"r\").read().splitlines()\n",
    "print(f\"Number of Stopwords in consideration: {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee28c50",
   "metadata": {},
   "source": [
    "## Fetch the Texts from the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8afe568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "depression = pd.read_json(\n",
    "    os.path.join(\n",
    "        DATA_PATH, \n",
    "        'Depression_Tweets', \n",
    "        'depression_json'\n",
    "    )\n",
    ")\n",
    "reddit = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_PATH, \n",
    "        'Reddit_SuicideWatch', \n",
    "        'reddit_suicidewatch.csv'\n",
    "    ), \n",
    "    encoding='utf-8'\n",
    ")\n",
    "social_media_sentiment_analysis = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_PATH, \n",
    "        'Social_Media_Sentiments_Analysis_Dataset', \n",
    "        'sentimentdataset_annotated_binary.csv'\n",
    "    ), \n",
    "    encoding='utf-8'\n",
    ")\n",
    "twitter_suicidal_data = pd.read_csv(\n",
    "    os.path.join(\n",
    "        DATA_PATH, \n",
    "        'Twitter_Suicidal_Data', \n",
    "        'twitter-suicidal_data.csv'\n",
    "    ), \n",
    "    encoding='utf-8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7488d3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['content'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depression.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3df1645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'selftext', 'author_fullname', 'title', 'hide_score',\n",
       "       'name', 'upvote_ratio', 'ups', 'author_flair_template_id', 'score',\n",
       "       'edited', 'author_flair_css_class', 'created', 'selftext_html',\n",
       "       'no_follow', 'over_18', 'id', 'author', 'num_comments',\n",
       "       'author_flair_text_color', 'permalink', 'url', 'created_utc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d22967a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Text', 'Sentiment', 'Timestamp', 'User',\n",
       "       'Platform', 'Hashtags', 'Retweets', 'Likes', 'Country', 'Year', 'Month',\n",
       "       'Day', 'Hour', 'Annotation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_media_sentiment_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b182ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'intention'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_suicidal_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9f92c",
   "metadata": {},
   "source": [
    "## N-Gram Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20942bf",
   "metadata": {},
   "source": [
    "Performs an n-gram analysis (from 2-gram to 5-gram) and exports results to corresponding `.jsonl` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82413eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(n: int=2, corpus: str=\"\", stopwords: list=[]):\n",
    "    pattern = re.compile(r'\\b\\w+\\b')\n",
    "    # filtered corpus should have no punctuations AND stopwords\n",
    "    filtered_corpus = [word for word in corpus if pattern.match(word) and word not in stopwords]\n",
    "    # Compute bigrams from the filtered corpus\n",
    "    ngrams = [tuple(filtered_corpus[i:i+n]) for i in range(len(filtered_corpus)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4c18eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram from Depression JSON dataset:\n",
      "\n",
      "Number of 2-gram from Depression JSON dataset: 7\n",
      "First 20 2-grams from Depression JSON dataset: [(('Yea', 'typically'), 1), (('typically', 'crying'), 1), (('crying', 'sign'), 1), (('sign', 'uncontrolled'), 1), (('uncontrolled', 'depression'), 1), (('depression', 'struggled'), 1), (('struggled', \"life'\"), 1)]\n",
      "2-gram from Reddit SuicideWatch dataset:\n",
      "\n",
      "Number of 2-gram from Reddit SuicideWatch dataset: 106\n",
      "First 20 2-grams from Reddit SuicideWatch dataset: [(('I', 'left'), 2), (('Help?', 'I'), 1), (('I', 'reached'), 1), (('reached', 'point'), 1), (('point', 'today.'), 1), (('today.', 'I'), 1), (('I', 'struggling'), 1), (('struggling', 'lot'), 1), (('lot', 'financially'), 1), (('financially', 'trying'), 1), (('trying', 'support'), 1), (('support', 'kids.'), 1), (('kids.', 'I'), 1), (('I', 'lose'), 1), (('lose', 'house.'), 1), (('house.', 'I'), 1), (('I', 'work'), 1), (('work', 'full'), 1), (('full', 'I'), 1), (('I', \"it's\"), 1)]\n",
      "2-gram from Social Media Sentiment Analysis dataset:\n",
      "\n",
      "Number of 2-gram from Social Media Sentiment Analysis dataset: 2\n",
      "First 20 2-grams from Social Media Sentiment Analysis dataset: [(('Enjoying', 'beautiful'), 1), (('beautiful', 'park!'), 1)]\n",
      "2-gram from Twitter Suicidal dataset:\n",
      "\n",
      "Number of 2-gram from Twitter Suicidal dataset: 31\n",
      "First 20 2-grams from Twitter Suicidal dataset: [(('life', 'meaningless'), 1), (('meaningless', 'life'), 1), (('life', 'badly'), 1), (('badly', 'life'), 1), (('life', 'completely'), 1), (('completely', 'empty'), 1), (('empty', 'dont'), 1), (('dont', 'create'), 1), (('create', 'meaning'), 1), (('meaning', 'creating'), 1), (('creating', 'meaning'), 1), (('meaning', 'pain'), 1), (('pain', 'hold'), 1), (('hold', 'back'), 1), (('back', 'urge'), 1), (('urge', 'run'), 1), (('run', 'car'), 1), (('car', 'head'), 1), (('head', 'person'), 1), (('person', 'coming'), 1)]\n",
      "3-gram from Depression JSON dataset:\n",
      "\n",
      "Number of 3-gram from Depression JSON dataset: 6\n",
      "First 20 3-grams from Depression JSON dataset: [(('Yea', 'typically', 'crying'), 1), (('typically', 'crying', 'sign'), 1), (('crying', 'sign', 'uncontrolled'), 1), (('sign', 'uncontrolled', 'depression'), 1), (('uncontrolled', 'depression', 'struggled'), 1), (('depression', 'struggled', \"life'\"), 1)]\n",
      "3-gram from Reddit SuicideWatch dataset:\n",
      "\n",
      "Number of 3-gram from Reddit SuicideWatch dataset: 105\n",
      "First 20 3-grams from Reddit SuicideWatch dataset: [(('Help?', 'I', 'reached'), 1), (('I', 'reached', 'point'), 1), (('reached', 'point', 'today.'), 1), (('point', 'today.', 'I'), 1), (('today.', 'I', 'struggling'), 1), (('I', 'struggling', 'lot'), 1), (('struggling', 'lot', 'financially'), 1), (('lot', 'financially', 'trying'), 1), (('financially', 'trying', 'support'), 1), (('trying', 'support', 'kids.'), 1), (('support', 'kids.', 'I'), 1), (('kids.', 'I', 'lose'), 1), (('I', 'lose', 'house.'), 1), (('lose', 'house.', 'I'), 1), (('house.', 'I', 'work'), 1), (('I', 'work', 'full'), 1), (('work', 'full', 'I'), 1), (('full', 'I', \"it's\"), 1), (('I', \"it's\", 'enough.'), 1), ((\"it's\", 'enough.', 'I'), 1)]\n",
      "3-gram from Social Media Sentiment Analysis dataset:\n",
      "\n",
      "Number of 3-gram from Social Media Sentiment Analysis dataset: 1\n",
      "First 20 3-grams from Social Media Sentiment Analysis dataset: [(('Enjoying', 'beautiful', 'park!'), 1)]\n",
      "3-gram from Twitter Suicidal dataset:\n",
      "\n",
      "Number of 3-gram from Twitter Suicidal dataset: 30\n",
      "First 20 3-grams from Twitter Suicidal dataset: [(('life', 'meaningless', 'life'), 1), (('meaningless', 'life', 'badly'), 1), (('life', 'badly', 'life'), 1), (('badly', 'life', 'completely'), 1), (('life', 'completely', 'empty'), 1), (('completely', 'empty', 'dont'), 1), (('empty', 'dont', 'create'), 1), (('dont', 'create', 'meaning'), 1), (('create', 'meaning', 'creating'), 1), (('meaning', 'creating', 'meaning'), 1), (('creating', 'meaning', 'pain'), 1), (('meaning', 'pain', 'hold'), 1), (('pain', 'hold', 'back'), 1), (('hold', 'back', 'urge'), 1), (('back', 'urge', 'run'), 1), (('urge', 'run', 'car'), 1), (('run', 'car', 'head'), 1), (('car', 'head', 'person'), 1), (('head', 'person', 'coming'), 1), (('person', 'coming', 'opposite'), 1)]\n",
      "4-gram from Depression JSON dataset:\n",
      "\n",
      "Number of 4-gram from Depression JSON dataset: 5\n",
      "First 20 4-grams from Depression JSON dataset: [(('Yea', 'typically', 'crying', 'sign'), 1), (('typically', 'crying', 'sign', 'uncontrolled'), 1), (('crying', 'sign', 'uncontrolled', 'depression'), 1), (('sign', 'uncontrolled', 'depression', 'struggled'), 1), (('uncontrolled', 'depression', 'struggled', \"life'\"), 1)]\n",
      "4-gram from Reddit SuicideWatch dataset:\n",
      "\n",
      "Number of 4-gram from Reddit SuicideWatch dataset: 104\n",
      "First 20 4-grams from Reddit SuicideWatch dataset: [(('Help?', 'I', 'reached', 'point'), 1), (('I', 'reached', 'point', 'today.'), 1), (('reached', 'point', 'today.', 'I'), 1), (('point', 'today.', 'I', 'struggling'), 1), (('today.', 'I', 'struggling', 'lot'), 1), (('I', 'struggling', 'lot', 'financially'), 1), (('struggling', 'lot', 'financially', 'trying'), 1), (('lot', 'financially', 'trying', 'support'), 1), (('financially', 'trying', 'support', 'kids.'), 1), (('trying', 'support', 'kids.', 'I'), 1), (('support', 'kids.', 'I', 'lose'), 1), (('kids.', 'I', 'lose', 'house.'), 1), (('I', 'lose', 'house.', 'I'), 1), (('lose', 'house.', 'I', 'work'), 1), (('house.', 'I', 'work', 'full'), 1), (('I', 'work', 'full', 'I'), 1), (('work', 'full', 'I', \"it's\"), 1), (('full', 'I', \"it's\", 'enough.'), 1), (('I', \"it's\", 'enough.', 'I'), 1), ((\"it's\", 'enough.', 'I', 'plan'), 1)]\n",
      "4-gram from Social Media Sentiment Analysis dataset:\n",
      "\n",
      "Number of 4-gram from Social Media Sentiment Analysis dataset: 0\n",
      "First 20 4-grams from Social Media Sentiment Analysis dataset: []\n",
      "4-gram from Twitter Suicidal dataset:\n",
      "\n",
      "Number of 4-gram from Twitter Suicidal dataset: 29\n",
      "First 20 4-grams from Twitter Suicidal dataset: [(('life', 'meaningless', 'life', 'badly'), 1), (('meaningless', 'life', 'badly', 'life'), 1), (('life', 'badly', 'life', 'completely'), 1), (('badly', 'life', 'completely', 'empty'), 1), (('life', 'completely', 'empty', 'dont'), 1), (('completely', 'empty', 'dont', 'create'), 1), (('empty', 'dont', 'create', 'meaning'), 1), (('dont', 'create', 'meaning', 'creating'), 1), (('create', 'meaning', 'creating', 'meaning'), 1), (('meaning', 'creating', 'meaning', 'pain'), 1), (('creating', 'meaning', 'pain', 'hold'), 1), (('meaning', 'pain', 'hold', 'back'), 1), (('pain', 'hold', 'back', 'urge'), 1), (('hold', 'back', 'urge', 'run'), 1), (('back', 'urge', 'run', 'car'), 1), (('urge', 'run', 'car', 'head'), 1), (('run', 'car', 'head', 'person'), 1), (('car', 'head', 'person', 'coming'), 1), (('head', 'person', 'coming', 'opposite'), 1), (('person', 'coming', 'opposite', 'feeling'), 1)]\n",
      "5-gram from Depression JSON dataset:\n",
      "\n",
      "Number of 5-gram from Depression JSON dataset: 4\n",
      "First 20 5-grams from Depression JSON dataset: [(('Yea', 'typically', 'crying', 'sign', 'uncontrolled'), 1), (('typically', 'crying', 'sign', 'uncontrolled', 'depression'), 1), (('crying', 'sign', 'uncontrolled', 'depression', 'struggled'), 1), (('sign', 'uncontrolled', 'depression', 'struggled', \"life'\"), 1)]\n",
      "5-gram from Reddit SuicideWatch dataset:\n",
      "\n",
      "Number of 5-gram from Reddit SuicideWatch dataset: 103\n",
      "First 20 5-grams from Reddit SuicideWatch dataset: [(('Help?', 'I', 'reached', 'point', 'today.'), 1), (('I', 'reached', 'point', 'today.', 'I'), 1), (('reached', 'point', 'today.', 'I', 'struggling'), 1), (('point', 'today.', 'I', 'struggling', 'lot'), 1), (('today.', 'I', 'struggling', 'lot', 'financially'), 1), (('I', 'struggling', 'lot', 'financially', 'trying'), 1), (('struggling', 'lot', 'financially', 'trying', 'support'), 1), (('lot', 'financially', 'trying', 'support', 'kids.'), 1), (('financially', 'trying', 'support', 'kids.', 'I'), 1), (('trying', 'support', 'kids.', 'I', 'lose'), 1), (('support', 'kids.', 'I', 'lose', 'house.'), 1), (('kids.', 'I', 'lose', 'house.', 'I'), 1), (('I', 'lose', 'house.', 'I', 'work'), 1), (('lose', 'house.', 'I', 'work', 'full'), 1), (('house.', 'I', 'work', 'full', 'I'), 1), (('I', 'work', 'full', 'I', \"it's\"), 1), (('work', 'full', 'I', \"it's\", 'enough.'), 1), (('full', 'I', \"it's\", 'enough.', 'I'), 1), (('I', \"it's\", 'enough.', 'I', 'plan'), 1), ((\"it's\", 'enough.', 'I', 'plan', 'today'), 1)]\n",
      "5-gram from Social Media Sentiment Analysis dataset:\n",
      "\n",
      "Number of 5-gram from Social Media Sentiment Analysis dataset: 0\n",
      "First 20 5-grams from Social Media Sentiment Analysis dataset: []\n",
      "5-gram from Twitter Suicidal dataset:\n",
      "\n",
      "Number of 5-gram from Twitter Suicidal dataset: 28\n",
      "First 20 5-grams from Twitter Suicidal dataset: [(('life', 'meaningless', 'life', 'badly', 'life'), 1), (('meaningless', 'life', 'badly', 'life', 'completely'), 1), (('life', 'badly', 'life', 'completely', 'empty'), 1), (('badly', 'life', 'completely', 'empty', 'dont'), 1), (('life', 'completely', 'empty', 'dont', 'create'), 1), (('completely', 'empty', 'dont', 'create', 'meaning'), 1), (('empty', 'dont', 'create', 'meaning', 'creating'), 1), (('dont', 'create', 'meaning', 'creating', 'meaning'), 1), (('create', 'meaning', 'creating', 'meaning', 'pain'), 1), (('meaning', 'creating', 'meaning', 'pain', 'hold'), 1), (('creating', 'meaning', 'pain', 'hold', 'back'), 1), (('meaning', 'pain', 'hold', 'back', 'urge'), 1), (('pain', 'hold', 'back', 'urge', 'run'), 1), (('hold', 'back', 'urge', 'run', 'car'), 1), (('back', 'urge', 'run', 'car', 'head'), 1), (('urge', 'run', 'car', 'head', 'person'), 1), (('run', 'car', 'head', 'person', 'coming'), 1), (('car', 'head', 'person', 'coming', 'opposite'), 1), (('head', 'person', 'coming', 'opposite', 'feeling'), 1), (('person', 'coming', 'opposite', 'feeling', 'jealous'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the output directory exists\n",
    "output_dir = 'N-Gram Analysis'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "datasets = {\n",
    "    \"depression_ngram.json\": [],\n",
    "    \"reddit_ngram.json\": [],\n",
    "    \"social_media_sentiment_analysis_ngram.json\": [],\n",
    "    \"twitter_suicidal_ngram.json\": []\n",
    "}\n",
    "\n",
    "# Print and Export top 20 n-grams for each dataset\n",
    "for N in range(2, 6):  # bigram to 5-gram combinations\n",
    "    # Depression\n",
    "    print(f\"{N}-gram from Depression JSON dataset:\\n\")\n",
    "\n",
    "    ngrams_depression = get_n_gram(\n",
    "        n=N,\n",
    "        corpus=depression['content'][0].split(),\n",
    "        stopwords=stopwords\n",
    "    )\n",
    "\n",
    "    print(f\"Number of {N}-gram from Depression JSON dataset: {len(ngrams_depression)}\")\n",
    "\n",
    "    ngrams_depression_top_20 = Counter(ngrams_depression).most_common(20)\n",
    "    print(f\"First 20 {N}-grams from Depression JSON dataset: {ngrams_depression_top_20}\")\n",
    "        \n",
    "    # Transform to desired format\n",
    "    formatted_ngrams = [\n",
    "        {\"content\": \" \".join(ngram), \"N-gram\": N, \"frequency\": freq}\n",
    "        for ngram, freq in ngrams_depression_top_20\n",
    "    ]\n",
    "    datasets[\"depression_ngram.json\"].extend(formatted_ngrams)\n",
    "\n",
    "    # Reddit\n",
    "    print(f\"{N}-gram from Reddit SuicideWatch dataset:\\n\")\n",
    "\n",
    "    reddit['combined_text'] = reddit['title'] + \" \" + reddit['selftext']\n",
    "    ngrams_reddit = get_n_gram(\n",
    "        n=N,\n",
    "        corpus=reddit['combined_text'][0].split(),\n",
    "        stopwords=stopwords\n",
    "    )\n",
    "\n",
    "    print(f\"Number of {N}-gram from Reddit SuicideWatch dataset: {len(ngrams_reddit)}\")\n",
    "\n",
    "    ngrams_reddit_top_20 = Counter(ngrams_reddit).most_common(20)\n",
    "    print(f\"First 20 {N}-grams from Reddit SuicideWatch dataset: {ngrams_reddit_top_20}\")\n",
    "        \n",
    "    # Transform to desired format\n",
    "    formatted_ngrams = [\n",
    "        {\"content\": \" \".join(ngram), \"N-gram\": N, \"frequency\": freq}\n",
    "        for ngram, freq in ngrams_reddit_top_20\n",
    "    ]\n",
    "    datasets[\"reddit_ngram.json\"].extend(formatted_ngrams)\n",
    "\n",
    "    # Social Media Sentiment Analysis\n",
    "    print(f\"{N}-gram from Social Media Sentiment Analysis dataset:\\n\")\n",
    "\n",
    "    social_media_sentiment_analysis['combined_text'] = social_media_sentiment_analysis['Text'] + \" \" + social_media_sentiment_analysis['Hashtags']\n",
    "    ngrams_social_media_sentiment_analysis = get_n_gram(\n",
    "        n=N,\n",
    "        corpus=social_media_sentiment_analysis['combined_text'][0].split(),\n",
    "        stopwords=stopwords\n",
    "    )\n",
    "\n",
    "    print(f\"Number of {N}-gram from Social Media Sentiment Analysis dataset: {len(ngrams_social_media_sentiment_analysis)}\")\n",
    "\n",
    "    ngrams_social_media_sentiment_analysis_top_20 = Counter(ngrams_social_media_sentiment_analysis).most_common(20)\n",
    "    print(f\"First 20 {N}-grams from Social Media Sentiment Analysis dataset: {ngrams_social_media_sentiment_analysis_top_20}\")\n",
    "        \n",
    "    # Transform to desired format\n",
    "    formatted_ngrams = [\n",
    "        {\"content\": \" \".join(ngram), \"N-gram\": N, \"frequency\": freq}\n",
    "        for ngram, freq in ngrams_social_media_sentiment_analysis_top_20\n",
    "    ]\n",
    "    datasets[\"social_media_sentiment_analysis_ngram.json\"].extend(formatted_ngrams)\n",
    "\n",
    "    # Twitter Suicidal Data\n",
    "    print(f\"{N}-gram from Twitter Suicidal dataset:\\n\")\n",
    "\n",
    "    ngrams_twitter_suicidal_data = get_n_gram(\n",
    "        n=N,\n",
    "        corpus=twitter_suicidal_data['tweet'][0].split(),\n",
    "        stopwords=stopwords\n",
    "    )\n",
    "\n",
    "    print(f\"Number of {N}-gram from Twitter Suicidal dataset: {len(ngrams_twitter_suicidal_data)}\")\n",
    "\n",
    "    ngrams_twitter_suicidal_data_top_20 = Counter(ngrams_twitter_suicidal_data).most_common(20)\n",
    "    print(f\"First 20 {N}-grams from Twitter Suicidal dataset: {ngrams_twitter_suicidal_data_top_20}\")\n",
    "        \n",
    "    # Transform to desired format\n",
    "    formatted_ngrams = [\n",
    "        {\"content\": \" \".join(ngram), \"N-gram\": N, \"frequency\": freq}\n",
    "        for ngram, freq in ngrams_twitter_suicidal_data_top_20\n",
    "    ]\n",
    "    datasets[\"twitter_suicidal_ngram.json\"].extend(formatted_ngrams)\n",
    "\n",
    "# Export to JSON files\n",
    "for filename, data in datasets.items():\n",
    "    with open(os.path.join(output_dir, filename), 'a') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4c482",
   "metadata": {},
   "source": [
    "### N-gram Analysis based on Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f306bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = os.path.abspath(\n",
    "    os.path.join(\n",
    "        \"NLP Training\", \n",
    "        'Results'\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9de84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in Result_deep_learning_customized_standalone.jsonl: 18679\n",
      "Number of 2-gram from Result_deep_learning_customized_standalone.jsonl: 7\n",
      "First 20 2-grams from Result_deep_learning_customized_standalone.jsonl: [(('Yea', 'typically'), 1), (('typically', 'crying'), 1), (('crying', 'sign'), 1), (('sign', 'uncontrolled'), 1), (('uncontrolled', 'depression'), 1), (('depression', 'struggled'), 1), (('struggled', \"life'\"), 1)]\n",
      "\n",
      "\n",
      "Number of 3-gram from Result_deep_learning_customized_standalone.jsonl: 6\n",
      "First 20 3-grams from Result_deep_learning_customized_standalone.jsonl: [(('Yea', 'typically', 'crying'), 1), (('typically', 'crying', 'sign'), 1), (('crying', 'sign', 'uncontrolled'), 1), (('sign', 'uncontrolled', 'depression'), 1), (('uncontrolled', 'depression', 'struggled'), 1), (('depression', 'struggled', \"life'\"), 1)]\n",
      "\n",
      "\n",
      "Number of 4-gram from Result_deep_learning_customized_standalone.jsonl: 5\n",
      "First 20 4-grams from Result_deep_learning_customized_standalone.jsonl: [(('Yea', 'typically', 'crying', 'sign'), 1), (('typically', 'crying', 'sign', 'uncontrolled'), 1), (('crying', 'sign', 'uncontrolled', 'depression'), 1), (('sign', 'uncontrolled', 'depression', 'struggled'), 1), (('uncontrolled', 'depression', 'struggled', \"life'\"), 1)]\n",
      "\n",
      "\n",
      "Number of 5-gram from Result_deep_learning_customized_standalone.jsonl: 4\n",
      "First 20 5-grams from Result_deep_learning_customized_standalone.jsonl: [(('Yea', 'typically', 'crying', 'sign', 'uncontrolled'), 1), (('typically', 'crying', 'sign', 'uncontrolled', 'depression'), 1), (('crying', 'sign', 'uncontrolled', 'depression', 'struggled'), 1), (('sign', 'uncontrolled', 'depression', 'struggled', \"life'\"), 1)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for jsonFile in os.listdir(RESULTS_PATH):\n",
    "    if jsonFile.endswith('.json') or jsonFile.endswith('.jsonl'):\n",
    "        with open(os.path.join(RESULTS_PATH, jsonFile), 'r') as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "            print(f\"Number of records in {jsonFile}: {len(data)}\")\n",
    "            for N in range(2, 6):\n",
    "                ngram_results: list[tuple] = get_n_gram(\n",
    "                    n=N,\n",
    "                    corpus=data[0]['raw_text'].split(),\n",
    "                    stopwords=stopwords\n",
    "                )\n",
    "                print(f\"Number of {N}-gram from {jsonFile}: {len(ngram_results)}\")\n",
    "                ngram_results_top_20 = Counter(ngram_results).most_common(20)\n",
    "                print(f\"First 20 {N}-grams from {jsonFile}: {ngram_results_top_20}\")\n",
    "                print(\"\\n\")\n",
    "                # For exporting\n",
    "                if jsonFile not in results:\n",
    "                    results[jsonFile] = []\n",
    "                results[jsonFile].extend([\n",
    "                    {\"content\": \" \".join(ngram), \"N-gram\": N, \"frequency\": freq, \"predicted_label\": data[0]['predicted_label']}\n",
    "                    for ngram, freq in ngram_results_top_20\n",
    "                ])\n",
    "# Export to JSON files\n",
    "with open(os.path.join(output_dir, 'pred_results.json'), 'a') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c5db6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a863d3e",
   "metadata": {},
   "source": [
    "Analyze the Sentiments (i.e., Polarity and Subjectivity) of the top 20 n-grams in each dataset and updates the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf20915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def analyze_sentiment(record):\n",
    "    text = str(record['content'])\n",
    "    scores = sia.polarity_scores(text)\n",
    "    # Map VADER scores to a similar structure\n",
    "    record['polarity_VADER'] = scores['compound']  # Ranges from -1 to 1\n",
    "    record['positive'] = scores['pos']\n",
    "    record['negative'] = scores['neg']\n",
    "    record['neutral'] = scores['neu']\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3605618f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DEPRESSION dataset: Polarity and Subjectivity are computed.\n",
      "✅ depression_ngram.json updated with polarity and subjectivity\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# TextBlob\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     wiki \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[38;5;28mstr\u001b[39m(record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m---> 12\u001b[0m     record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolarity_TextBlob\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mwiki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[38;5;241m.\u001b[39mpolarity\n\u001b[0;32m     13\u001b[0m     record[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjectivity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m wiki\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39msubjectivity\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# VADER\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Data\\Canada\\Study\\masters\\CSI 5386 - Natural Language Processing\\Natural-Language-Processing\\project-venv\\lib\\site-packages\\textblob\\decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Data\\Canada\\Study\\masters\\CSI 5386 - Natural Language Processing\\Natural-Language-Processing\\project-venv\\lib\\site-packages\\textblob\\blob.py:439\u001b[0m, in \u001b[0;36mBaseBlob.sentiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msentiment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a tuple of form (polarity, subjectivity ) where polarity\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    is a float within the range [-1.0, 1.0] and subjectivity is a float\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    :rtype: namedtuple of the form ``Sentiment(polarity, subjectivity)``\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Data\\Canada\\Study\\masters\\CSI 5386 - Natural Language Processing\\Natural-Language-Processing\\project-venv\\lib\\site-packages\\textblob\\en\\sentiments.py:44\u001b[0m, in \u001b[0;36mPatternAnalyzer.analyze\u001b[1;34m(self, text, keep_assessments)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sentiment(polarity, subjectivity, assessments)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 44\u001b[0m     Sentiment \u001b[38;5;241m=\u001b[39m \u001b[43mnamedtuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpolarity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubjectivity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sentiment(\u001b[38;5;241m*\u001b[39mpattern_sentiment(text))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\collections\\__init__.py:490\u001b[0m, in \u001b[0;36mnamedtuple\u001b[1;34m(typename, field_names, rename, defaults, module)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mf_globals\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ngram_analysis_sets:dict = {}\n",
    "\n",
    "for ngram_result_file in os.listdir(os.path.join(output_dir)):\n",
    "    with open(os.path.join(output_dir, ngram_result_file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        ngram_analysis_sets[ngram_result_file] = data\n",
    "        if (isinstance(data, dict)):\n",
    "            for key, value in data.items():\n",
    "                for record in value:\n",
    "                    # TextBlob\n",
    "                    wiki = TextBlob(str(record['content']))\n",
    "                    record['polarity_TextBlob'] = wiki.sentiment.polarity\n",
    "                    record['subjectivity'] = wiki.sentiment.subjectivity\n",
    "                    # VADER\n",
    "                    wiki = analyze_sentiment(record)\n",
    "                    ngram_analysis_sets[ngram_result_file][key].append(wiki)\n",
    "        if (isinstance(data, list)):\n",
    "            for idx, record in enumerate(data):\n",
    "                # TextBlob\n",
    "                wiki = TextBlob(str(record['content']))\n",
    "                record['polarity_TextBlob'] = wiki.sentiment.polarity\n",
    "                record['subjectivity'] = wiki.sentiment.subjectivity\n",
    "                # VADER\n",
    "                wiki = analyze_sentiment(record)\n",
    "                ngram_analysis_sets[ngram_result_file][idx] = wiki\n",
    "        print(f\"✅ {ngram_result_file.split('.json')[0].split('_ngram')[0].upper()} dataset: Polarity and Subjectivity are computed.\")\n",
    "\n",
    "    # Export updates to JSON files\n",
    "    with open(os.path.join(output_dir, ngram_result_file), 'w') as f:\n",
    "        json.dump(ngram_analysis_sets[ngram_result_file], f, indent=4)\n",
    "        print(f\"✅ {ngram_result_file} updated with polarity and subjectivity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb0aa09",
   "metadata": {},
   "source": [
    "**Polarity** from TextBlob:\n",
    "* A float within the range [-1.0, 1.0]. A score of -1 means the words are super negative, like “disgusting” or “awful.” A score of 1 means the words are super positive, like “excellent” or “best.”\n",
    "\n",
    "**Subjectivity** from TextBlob:\n",
    "* A float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "\n",
    "Source: <url>https://textblob.readthedocs.io/en/dev/quickstart.html</url>\n",
    "\n",
    "**Polarity** from VADER:\n",
    "* A float within the range [-1.0, 1.0]. A score of -1 means the words are super negative. A score of 1 means the words are super positive, like “excellent” or “best.”\n",
    "\n",
    "**Positivity** from VADER:\n",
    "* A float within the range [0.0, 1.0] indicating the proportion of Positive sentiments.\n",
    "\n",
    "**Negativity** from VADER:\n",
    "* A float within the range [0.0, 1.0] indicating the proportion of Negative sentiments.\n",
    "\n",
    "**Neutral Score** from VADER:\n",
    "* A float within the range [0.0, 1.0] indicating the proportion of Neutral sentiments.\n",
    "\n",
    "Source: <url>https://www.nltk.org/api/nltk.html</url>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
