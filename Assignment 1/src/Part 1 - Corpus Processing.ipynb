{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Corpus processing (legal text): tokenization and word counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_tokenizer\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted .txt files (excluding README): 510\n",
      "Number of extracted README files: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the zip file and the extraction directory\n",
    "parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "zip_file_path = os.path.join(parent_path, 'CUAD_v1.zip')\n",
    "extraction_dir = 'extracted_txt_files'\n",
    "\n",
    "# Check if the zip file exists\n",
    "if not os.path.exists(zip_file_path):\n",
    "    print(f\"Error: The file {zip_file_path} does not exist.\")\n",
    "else:\n",
    "    # Delete the extraction directory if it exists\n",
    "    if os.path.exists(extraction_dir):\n",
    "        shutil.rmtree(extraction_dir)\n",
    "\n",
    "    # Create the extraction directory\n",
    "    os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "    # Extract only .txt files\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith('.txt'):\n",
    "                zip_ref.extract(file, extraction_dir)\n",
    "\n",
    "    # Verify the number of extracted .txt files, excluding README files\n",
    "    extracted_files = []\n",
    "    readme_files = []\n",
    "    for root, dirs, files in os.walk(extraction_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                if \"README\" in file:\n",
    "                    readme_files.append(os.path.join(root, file))\n",
    "                else:\n",
    "                    extracted_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Number of extracted .txt files (excluding README): {len(extracted_files)}\")\n",
    "    print(f\"Number of extracted README files: {len(readme_files)}\")\n",
    "    assert len(extracted_files) == 510, f\"Expected 510 text files, but found {len(extracted_files)}\"\n",
    "    assert len(readme_files) == 1, f\"Expected 1 README file, but found {len(readme_files)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Files to form a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Found: Corpus created with 4087261 words.\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "tokenizer = word_tokenizer.WordTokenizer()\n",
    "\n",
    "for file_path in extracted_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus.extend(tokenizer.tokenize(file.read()))\n",
    "\n",
    "# update the tokenizer with the corpus\n",
    "tokenizer.corpus = ' '.join(corpus)\n",
    "\n",
    "# total words\n",
    "total_words = len(corpus)\n",
    "\n",
    "print(f\"Tokens Found: Corpus created with {total_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Types (i.e., unique words): 45597\n"
     ]
    }
   ],
   "source": [
    "word_counts: dict = tokenizer.countOccurrences(text=tokenizer.corpus)\n",
    "\n",
    "# Number of Unique Words\n",
    "num_types = len(word_counts)\n",
    "print(f\"Number of Types (i.e., unique words): {num_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 0.011155881652774315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {num_types/total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the Frequency of each Token in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and their frequencies have been written to tokens.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert word_counts to a Counter object\n",
    "word_counts_counter = Counter(word_counts)\n",
    "\n",
    "# Delete the file if it exists\n",
    "tokens_file_path = 'tokens.txt'\n",
    "if os.path.exists(tokens_file_path):\n",
    "    os.remove(tokens_file_path)\n",
    "\n",
    "# Write the tokens and their frequencies to tokens.txt\n",
    "with open(tokens_file_path, 'w', encoding='utf-8') as tokens_file:\n",
    "    for token, frequency in word_counts_counter.most_common(): # desc order frequency\n",
    "        tokens_file.write(f\"{token}: {frequency}\\n\")\n",
    "\n",
    "print(\"Tokens and their frequencies have been written to tokens.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze statistics about Tokens in the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract First 20 Tokens (Words) onto a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 Sample word counts: {'CO': 202, 'BRANDING': 53, 'AND': 3761, 'ADVERTISING': 72, 'AGREEMENT': 2055, 'THIS': 1235, 'the': 239999, 'Agreement': 37020, 'is': 21544, 'made': 3865, 'as': 31637, 'of': 151815, 'June': 283, '21': 1275, '1999': 297, 'Effective': 2423, 'Date': 5406, 'by': 42050, 'and': 128998, 'between': 3492}\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 20 Sample word counts: {dict(list(word_counts_counter.items())[:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words have been written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Extract the first 20 words\n",
    "first_20_words = corpus[:20]\n",
    "\n",
    "# Delete the file if it exists\n",
    "output_file_path = 'output.txt'\n",
    "if os.path.exists(output_file_path):\n",
    "    os.remove(output_file_path)\n",
    "\n",
    "# Write the first 20 words to output.txt\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('\\n'.join(first_20_words))\n",
    "\n",
    "print(\"First 20 words have been written to output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tokens Appearing Once Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens Appearing Once Only: 15297\n"
     ]
    }
   ],
   "source": [
    "tokens_once = [word for word, count in word_counts_counter.items() if count == 1]\n",
    "num_tokens_once = len(tokens_once)\n",
    "\n",
    "print(f\"Number of Tokens Appearing Once Only: {num_tokens_once}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Only Words (Without Punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Corpus Length (excluding punctuations): 4087261\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match words\n",
    "pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# Filter the corpus to include only words\n",
    "filtered_corpus = [word for word in corpus if pattern.match(word)]\n",
    "\n",
    "print(f\"Filtered Corpus Length (excluding punctuations): {len(filtered_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Words: [('the', 239999), ('of', 151815), ('and', 128998), ('to', 127311), ('or', 106443), ('in', 74269), ('any', 58853), ('shall', 48424), ('a', 46609), ('by', 42050), ('be', 39165), ('Agreement', 37020), ('for', 35480), ('this', 35216), ('such', 34815), ('with', 32574), ('as', 31637), ('that', 27281), ('other', 25063), ('1', 23056)]\n"
     ]
    }
   ],
   "source": [
    "# List the top 20 most frequent words\n",
    "word_counts: dict = tokenizer.countOccurrences(text=' '.join(filtered_corpus))\n",
    "word_counts_counter = Counter(word_counts)\n",
    "top_20_words = word_counts_counter.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Words: {top_20_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {len(filtered_corpus)/len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stopwords considered: 779\n"
     ]
    }
   ],
   "source": [
    "stopwordsFilePath = 'StopWords.txt'\n",
    "stopwordsFile = open(stopwordsFilePath, 'r')\n",
    "stopwords:list = [line.strip() for line in stopwordsFile.readlines()]\n",
    "print(f\"Number of Stopwords considered: {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Corpus Length (excluding stopwords): 2206679\n"
     ]
    }
   ],
   "source": [
    "filtered_corpus = [word for word in corpus if word not in stopwords]\n",
    "print(f\"Filtered Corpus Length (excluding stopwords): {len(filtered_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Words (excluding stopwords): [('Agreement', 37020), ('1', 23056), ('Party', 19216), ('2', 18105), ('3', 14705), ('The', 13566), ('Section', 12406), ('party', 11045), ('4', 10626), ('Company', 9941), ('5', 9434), ('Product', 8852), ('Parties', 7685), ('6', 7362), ('10', 6915), ('set', 6873), ('A', 6779), ('written', 6735), ('8', 6528), ('applicable', 6477)]\n"
     ]
    }
   ],
   "source": [
    "# Top 20 Frequent Words after removing stopwords\n",
    "word_counts_no_stopwords: dict = tokenizer.countOccurrences(text=' '.join(filtered_corpus))\n",
    "word_counts_no_stopwords_counter = Counter(word_counts_no_stopwords)\n",
    "top_20_words_no_stopwords = word_counts_no_stopwords_counter.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Words (excluding stopwords): {top_20_words_no_stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 0.5398918738000827\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {len(filtered_corpus)/len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams (excluding Punctuations and Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bigrams: 2206678\n",
      "First 20 Bigrams: [('CO', 'BRANDING'), ('BRANDING', 'AND'), ('AND', 'ADVERTISING'), ('ADVERTISING', 'AGREEMENT'), ('AGREEMENT', 'THIS'), ('THIS', 'CO'), ('CO', 'BRANDING'), ('BRANDING', 'AND'), ('AND', 'ADVERTISING'), ('ADVERTISING', 'AGREEMENT'), ('AGREEMENT', 'Agreement'), ('Agreement', 'June'), ('June', '21'), ('21', '1999'), ('1999', 'Effective'), ('Effective', 'Date'), ('Date', 'I'), ('I', 'ESCROW'), ('ESCROW', 'INC'), ('INC', 'principal')]\n"
     ]
    }
   ],
   "source": [
    "# filtered corpus should have no punctuations AND stopwords\n",
    "filtered_corpus = [word for word in corpus if pattern.match(word) and word not in stopwords]\n",
    "\n",
    "# Compute bigrams from the filtered corpus\n",
    "bigrams = [(filtered_corpus[i], filtered_corpus[i+1]) for i in range(len(filtered_corpus)-1)]\n",
    "\n",
    "print(f\"Number of Bigrams: {len(bigrams)}\")\n",
    "print(f\"First 20 Bigrams: {bigrams[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Words (excluding punctuations and stopwords): [('Agreement', 37020), ('1', 23056), ('Party', 19216), ('2', 18105), ('3', 14705), ('The', 13566), ('Section', 12406), ('party', 11045), ('4', 10626), ('Company', 9941), ('5', 9434), ('Product', 8852), ('Parties', 7685), ('6', 7362), ('10', 6915), ('set', 6873), ('A', 6779), ('written', 6735), ('8', 6528), ('applicable', 6477)]\n"
     ]
    }
   ],
   "source": [
    "# Top 20 Frequent Words from the filtered corpus\n",
    "word_counts_bigram: dict = tokenizer.countOccurrences(text=' '.join(filtered_corpus))\n",
    "word_counts_bigrams_counter = Counter(word_counts_bigram)\n",
    "top_20_words_bigrams = word_counts_bigrams_counter.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Words (excluding punctuations and stopwords): {top_20_words_bigrams}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
