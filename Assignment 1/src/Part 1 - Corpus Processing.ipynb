{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Corpus processing (legal text): tokenization and word counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word_tokenizer\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Corpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted .txt files (excluding README): 510\n",
      "Number of extracted README files: 1\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the zip file and the extraction directory\n",
    "parent_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "zip_file_path = os.path.join(parent_path, 'CUAD_v1.zip')\n",
    "extraction_dir = 'extracted_txt_files'\n",
    "\n",
    "# Check if the zip file exists\n",
    "if not os.path.exists(zip_file_path):\n",
    "    print(f\"Error: The file {zip_file_path} does not exist.\")\n",
    "else:\n",
    "    # Delete the extraction directory if it exists\n",
    "    if os.path.exists(extraction_dir):\n",
    "        shutil.rmtree(extraction_dir)\n",
    "\n",
    "    # Create the extraction directory\n",
    "    os.makedirs(extraction_dir, exist_ok=True)\n",
    "\n",
    "    # Extract only .txt files\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.endswith('.txt'):\n",
    "                zip_ref.extract(file, extraction_dir)\n",
    "\n",
    "    # Verify the number of extracted .txt files, excluding README files\n",
    "    extracted_files = []\n",
    "    readme_files = []\n",
    "    for root, dirs, files in os.walk(extraction_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                if \"README\" in file:\n",
    "                    readme_files.append(os.path.join(root, file))\n",
    "                else:\n",
    "                    extracted_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Number of extracted .txt files (excluding README): {len(extracted_files)}\")\n",
    "    print(f\"Number of extracted README files: {len(readme_files)}\")\n",
    "    assert len(extracted_files) == 510, f\"Expected 510 text files, but found {len(extracted_files)}\"\n",
    "    assert len(readme_files) == 1, f\"Expected 1 README file, but found {len(readme_files)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Files to form a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Found: Corpus created with 4087261 words.\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "tokenizer = word_tokenizer.WordTokenizer()\n",
    "\n",
    "for file_path in extracted_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        corpus.extend(tokenizer.tokenize(file.read()))\n",
    "\n",
    "# update the tokenizer with the corpus\n",
    "tokenizer.corpus = ' '.join(corpus)\n",
    "\n",
    "# total words\n",
    "total_words = len(corpus)\n",
    "\n",
    "print(f\"Tokens Found: Corpus created with {total_words} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Types (i.e., unique words): 45597\n"
     ]
    }
   ],
   "source": [
    "word_counts: dict = tokenizer.countOccurrences(text=tokenizer.corpus)\n",
    "\n",
    "# Number of Unique Words\n",
    "num_types = len(word_counts)\n",
    "print(f\"Number of Types (i.e., unique words): {num_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 0.011155881652774315\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {num_types/total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record the Frequency of each Token in the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and their frequencies have been written to tokens.txt\n"
     ]
    }
   ],
   "source": [
    "# Convert word_counts to a Counter object\n",
    "word_counts_counter = Counter(word_counts)\n",
    "\n",
    "# Delete the file if it exists\n",
    "tokens_file_path = 'tokens.txt'\n",
    "if os.path.exists(tokens_file_path):\n",
    "    os.remove(tokens_file_path)\n",
    "\n",
    "# Write the tokens and their frequencies to tokens.txt\n",
    "with open(tokens_file_path, 'w', encoding='utf-8') as tokens_file:\n",
    "    for token, frequency in word_counts_counter.most_common(): # desc order frequency\n",
    "        tokens_file.write(f\"{token}: {frequency}\\n\")\n",
    "\n",
    "print(\"Tokens and their frequencies have been written to tokens.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze statistics about Tokens in the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract First 20 Tokens (Words) onto a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 Sample word counts: {'CO': 202, 'BRANDING': 53, 'AND': 3761, 'ADVERTISING': 72, 'AGREEMENT': 2055, 'THIS': 1235, 'the': 239999, 'Agreement': 37020, 'is': 21544, 'made': 3865, 'as': 31637, 'of': 151815, 'June': 283, '21': 1275, '1999': 297, 'Effective': 2423, 'Date': 5406, 'by': 42050, 'and': 128998, 'between': 3492}\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 20 Sample word counts: {dict(list(word_counts_counter.items())[:20])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire tokenized corpus has been written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Delete the file if it exists\n",
    "output_file_path = 'output.txt'\n",
    "if os.path.exists(output_file_path):\n",
    "    os.remove(output_file_path)\n",
    "\n",
    "# Write the entire tokenized corpus to output.txt\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write('\\n'.join(corpus))\n",
    "\n",
    "print(\"Entire tokenized corpus has been written to output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Tokens Appearing Once Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens Appearing Once Only: 15297\n"
     ]
    }
   ],
   "source": [
    "tokens_once = [word for word, count in word_counts_counter.items() if count == 1]\n",
    "num_tokens_once = len(tokens_once)\n",
    "\n",
    "print(f\"Number of Tokens Appearing Once Only: {num_tokens_once}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Only Words (Without Punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Corpus Length (excluding punctuations): 20710584\n"
     ]
    }
   ],
   "source": [
    "# Define a regular expression pattern to match words\n",
    "pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# Filter the corpus to include only words\n",
    "filtered_corpus = [word for word in tokenizer.corpus if pattern.match(word)]\n",
    "\n",
    "print(f\"Filtered Corpus Length (excluding punctuations): {len(filtered_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Words: [('e', 2386519), ('t', 1838526), ('i', 1492772), ('n', 1476250), ('a', 1454829), ('o', 1436395), ('r', 1372297), ('s', 1135258), ('h', 757883), ('l', 746439), ('c', 690049), ('d', 672764), ('u', 497405), ('m', 446608), ('f', 432631), ('p', 423939), ('y', 343846), ('g', 327485), ('b', 257295), ('v', 185444)]\n"
     ]
    }
   ],
   "source": [
    "# List the top 20 most frequent words\n",
    "word_counts: dict = tokenizer.countOccurrences(text=' '.join(filtered_corpus))\n",
    "word_counts_counter = Counter(word_counts)\n",
    "top_20_words = word_counts_counter.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Words: {top_20_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 0.8345288397539471\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {len(filtered_corpus)/len(tokenizer.corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stopwords considered: 779\n"
     ]
    }
   ],
   "source": [
    "stopwordsFilePath = 'StopWords.txt'\n",
    "stopwordsFile = open(stopwordsFilePath, 'r')\n",
    "stopwords:list = [line.strip() for line in stopwordsFile.readlines()]\n",
    "print(f\"Number of Stopwords considered: {len(stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Corpus Length (excluding stopwords): 2206679\n"
     ]
    }
   ],
   "source": [
    "filtered_corpus = [word for word in corpus if word not in stopwords]\n",
    "print(f\"Filtered Corpus Length (excluding stopwords): {len(filtered_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Words (excluding stopwords): [('Agreement', 37020), ('1', 23056), ('Party', 19216), ('2', 18105), ('3', 14705), ('The', 13566), ('Section', 12406), ('party', 11045), ('4', 10626), ('Company', 9941), ('5', 9434), ('Product', 8852), ('Parties', 7685), ('6', 7362), ('10', 6915), ('set', 6873), ('A', 6779), ('written', 6735), ('8', 6528), ('applicable', 6477)]\n"
     ]
    }
   ],
   "source": [
    "# Top 20 Frequent Words after removing stopwords\n",
    "word_counts_no_stopwords: dict = tokenizer.countOccurrences(text=' '.join(filtered_corpus))\n",
    "word_counts_no_stopwords_counter = Counter(word_counts_no_stopwords)\n",
    "top_20_words_no_stopwords = word_counts_no_stopwords_counter.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Words (excluding stopwords): {top_20_words_no_stopwords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type/Token Ratio: 0.5398918738000827\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type/Token Ratio: {len(filtered_corpus)/len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams (excluding Punctuations and Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bigrams: 2206678\n",
      "First 20 Bigrams: [('CO', 'BRANDING'), ('BRANDING', 'AND'), ('AND', 'ADVERTISING'), ('ADVERTISING', 'AGREEMENT'), ('AGREEMENT', 'THIS'), ('THIS', 'CO'), ('CO', 'BRANDING'), ('BRANDING', 'AND'), ('AND', 'ADVERTISING'), ('ADVERTISING', 'AGREEMENT'), ('AGREEMENT', 'Agreement'), ('Agreement', 'June'), ('June', '21'), ('21', '1999'), ('1999', 'Effective'), ('Effective', 'Date'), ('Date', 'I'), ('I', 'ESCROW'), ('ESCROW', 'INC'), ('INC', 'principal')]\n"
     ]
    }
   ],
   "source": [
    "# filtered corpus should have no punctuations AND stopwords\n",
    "filtered_corpus = [word for word in corpus if pattern.match(word) and word not in stopwords]\n",
    "\n",
    "# Compute bigrams from the filtered corpus\n",
    "bigrams = [(filtered_corpus[i], filtered_corpus[i+1]) for i in range(len(filtered_corpus)-1)]\n",
    "\n",
    "print(f\"Number of Bigrams: {len(bigrams)}\")\n",
    "print(f\"First 20 Bigrams: {bigrams[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Frequent Bigrams (excluding punctuations and stopwords): [('Confidential Information', 2869), ('written notice', 2369), ('Effective Date', 2264), ('This Agreement', 2250), ('In event', 2141), ('Third Party', 2012), ('terms conditions', 1902), ('prior written', 1807), ('set Section', 1749), ('1 1', 1682), ('Intellectual Property', 1636), ('2 1', 1499), ('Section 2', 1436), ('written consent', 1323), ('pursuant Section', 1307), ('30 days', 1285), ('United States', 1256), ('U S', 1254), ('2 2', 1240), ('Section 3', 1197)]\n"
     ]
    }
   ],
   "source": [
    "# Top 20 Frequent Bigrams from the filtered corpus\n",
    "bigram_strings = [' '.join(bigram) for bigram in bigrams]\n",
    "bigram_counts = Counter(bigram_strings)\n",
    "top_20_bigrams = bigram_counts.most_common(20)\n",
    "print(f\"Top 20 Most Frequent Bigrams (excluding punctuations and stopwords): {top_20_bigrams}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
