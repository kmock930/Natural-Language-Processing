{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b596f686-785d-4c3c-9428-7278ed8f2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: STS_Data\\STS2016.input.answer-answer.txt | STS_Data\\STS2016.gs.answer-answer.txt\n",
      "Processing: STS_Data\\STS2016.input.headlines.txt | STS_Data\\STS2016.gs.headlines.txt\n",
      "Processing: STS_Data\\STS2016.input.plagiarism.txt | STS_Data\\STS2016.gs.plagiarism.txt\n",
      "Processing: STS_Data\\STS2016.input.postediting.txt | STS_Data\\STS2016.gs.postediting.txt\n",
      "Processing: STS_Data\\STS2016.input.question-question.txt | STS_Data\\STS2016.gs.question-question.txt\n",
      "CSV file successfully generated: output\\STS_test_data.csv\n",
      "Total sentence pairs processed: 9183\n",
      "Dataset shape: (1186, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def process_files(input_files, gold_files, output_file):\n",
    "    sentences1, sentences2, gold_labels = [], [], []\n",
    "\n",
    "    for input_file, gold_file in zip(input_files, gold_files):\n",
    "        print(f\"Processing: {input_file} | {gold_file}\")\n",
    "        if not os.path.exists(input_file) or not os.path.exists(gold_file):\n",
    "            raise FileNotFoundError(f\"File not found: {input_file} or {gold_file}\")\n",
    "\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(gold_file, \"r\", encoding=\"utf-8\") as goldfile:\n",
    "            input_lines, gold_lines = infile.readlines(), goldfile.readlines()\n",
    "            min_length = min(len(input_lines), len(gold_lines))\n",
    "\n",
    "            for line in input_lines[:min_length]:\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                if len(fields) >= 2:\n",
    "                    sentences1.append(fields[0])\n",
    "                    sentences2.append(fields[1])\n",
    "\n",
    "            for line in gold_lines[:min_length]:\n",
    "                try:\n",
    "                    gold_labels.append(float(line.strip()))\n",
    "                except ValueError:\n",
    "                    pass  # Skip invalid data\n",
    "\n",
    "    min_length = min(len(sentences1), len(sentences2), len(gold_labels))\n",
    "    data = pd.DataFrame({\n",
    "        \"sentence1\": sentences1[:min_length],\n",
    "        \"sentence2\": sentences2[:min_length],\n",
    "        \"gold_label\": gold_labels[:min_length]\n",
    "    })\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    data.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "    print(f\"CSV file successfully generated: {output_file}\")\n",
    "    print(f\"Total sentence pairs processed: {len(sentences1)}\")\n",
    "    print(f\"Dataset shape: {data.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"STS_Data\"\n",
    "    input_files = [\n",
    "        os.path.join(base_dir, \"STS2016.input.answer-answer.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.input.headlines.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.input.plagiarism.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.input.postediting.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.input.question-question.txt\")\n",
    "    ]\n",
    "    gold_files = [\n",
    "        os.path.join(base_dir, \"STS2016.gs.answer-answer.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.gs.headlines.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.gs.plagiarism.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.gs.postediting.txt\"),\n",
    "        os.path.join(base_dir, \"STS2016.gs.question-question.txt\")\n",
    "    ]\n",
    "    output_file = os.path.join(\"output\", \"STS_test_data.csv\")\n",
    "\n",
    "    process_files(input_files, gold_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29550e-3aa9-48a3-b151-b951392abde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: stsb-roberta-large\n",
      "Model: stsb-roberta-large, Pearson Correlation: -0.0574\n",
      "Using model: paraphrase-MiniLM-L6-v2\n",
      "Model: paraphrase-MiniLM-L6-v2, Pearson Correlation: -0.0244\n",
      "Using model: bert-base-nli-mean-tokens\n",
      "Model: bert-base-nli-mean-tokens, Pearson Correlation: -0.0435\n",
      "Using model: all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_similarity(sts_file, output_file):\n",
    "    if not os.path.exists(sts_file):\n",
    "        raise FileNotFoundError(f\"STS test data file not found: {sts_file}\")\n",
    "\n",
    "    sts_data = pd.read_csv(sts_file)\n",
    "\n",
    "    if not all(col in sts_data.columns for col in ['sentence1', 'sentence2', 'gold_label']):\n",
    "        raise ValueError(\"Missing required columns in STS dataset.\")\n",
    "\n",
    "    models = {\n",
    "        \"stsb-roberta-large\": SentenceTransformer('stsb-roberta-large'),\n",
    "        \"paraphrase-MiniLM-L6-v2\": SentenceTransformer('paraphrase-MiniLM-L6-v2'),\n",
    "        \"bert-base-nli-mean-tokens\": SentenceTransformer('bert-base-nli-mean-tokens'),\n",
    "        \"all-mpnet-base-v2\": SentenceTransformer('all-mpnet-base-v2'),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Using model: {model_name}\")\n",
    "        similarities = []\n",
    "\n",
    "        for s1, s2 in zip(sts_data['sentence1'], sts_data['sentence2']):\n",
    "            embedding1 = model.encode(s1, convert_to_tensor=True)\n",
    "            embedding2 = model.encode(s2, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(embedding1, embedding2).item()\n",
    "            similarities.append(sim)\n",
    "\n",
    "        min_sim, max_sim = min(similarities), max(similarities)\n",
    "        normalized_similarities = [(sim - min_sim) / (max_sim - min_sim) * 5 for sim in similarities]\n",
    "\n",
    "        col_name = f'normalized_similarity_{model_name}'\n",
    "        sts_data[col_name] = normalized_similarities\n",
    "\n",
    "        pearson_corr, _ = pearsonr(normalized_similarities, sts_data['gold_label'])\n",
    "        results[model_name] = pearson_corr\n",
    "        print(f\"Model: {model_name}, Pearson Correlation: {pearson_corr:.4f}\")\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    sts_data.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nResults saved to: {output_file}\")\n",
    "\n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    for model_name, pearson_corr in results.items():\n",
    "        print(f\" Model: {model_name}, Pearson Correlation: {pearson_corr:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sts_file = \"output/STS_test_data.csv\"\n",
    "    output_file = \"output/STS_results_with_normalized_predictions.csv\"\n",
    "\n",
    "    compute_similarity(sts_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21277dac-6b61-4bb3-a247-2dd42c48752f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
