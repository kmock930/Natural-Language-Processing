{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1167669a-5d53-4557-81ff-006ea067f220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Zero-Shot Classification approach for LLM text detection\n",
      "Loading test data...\n",
      "Initializing zero-shot classifier...\n",
      "WARNING:tensorflow:From C:\\Users\\CW\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34272/34272 [24:25:56<00:00,  2.57s/it]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./content/Result_llm.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import constants\n",
    "from data_extraction import get_raw_dataset\n",
    "\n",
    "def save_predictions(ids, predictions, output_file='./content/Result_llm.jsonl'):\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    for id_, pred in zip(ids, predictions):\n",
    "        results.append({\"id\": id_, \"label\": int(pred)})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in results:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")a\n",
    "\n",
    "def zero_shot_classification():\n",
    "    \n",
    "    print(\"Loading test data...\")\n",
    "    X_test, ids_test = get_raw_dataset(mode='test')\n",
    "    \n",
    "    print(\"Initializing zero-shot classifier...\")\n",
    "    # Use a small model for zero-shot classification\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\", \n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=0  # Use GPU (0), can be changed to -1 to use CPU if memory issues occur\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print(\"Processing test data...\")\n",
    "    # Use tqdm to display progress bar\n",
    "    for text in tqdm(X_test):\n",
    "        # Limit text length to avoid token limits\n",
    "        truncated_text = text[:500]\n",
    "        \n",
    "        # Use zero-shot classification\n",
    "        output = classifier(\n",
    "            truncated_text,\n",
    "            candidate_labels=[\"human-written\", \"machine-generated\"],\n",
    "        )\n",
    "        \n",
    "        # Map results to expected labels (0 for human, 1 for machine)\n",
    "        pred_label = 1 if output[\"labels\"][0] == \"machine-generated\" else 0\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    # Save results\n",
    "    save_predictions(ids_test, predictions)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Zero-Shot Classification approach for LLM text detection\")\n",
    "    zero_shot_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7065cbae-63d0-4e9a-82e6-6220dcdc0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Zero-Shot Classification approach for LLM text detection\n",
      "Predictions file ./content/Result_llm.jsonl already exists.\n",
      "Loaded 34272 predictions for test set\n",
      "Loaded 34272 ground truth labels\n",
      "\n",
      "Test Set Evaluation:\n",
      "Evaluation Results:\n",
      "Accuracy: 0.5085\n",
      "Macro F1: 0.4358\n",
      "Micro F1: 0.5085\n",
      "Predictions saved to predictions/LLM_y_test_pred.npy\n",
      "Ground truth labels saved to predictions/LLM_y_test.npy\n",
      "\n",
      "Processing validation data...\n",
      "Loaded 5000 validation samples\n",
      "Making predictions on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|██████████| 5000/5000 [1:03:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Evaluation:\n",
      "Evaluation Results:\n",
      "Accuracy: 0.5206\n",
      "Macro F1: 0.4245\n",
      "Micro F1: 0.5206\n",
      "Saved validation predictions to predictions/LLM_y_dev_pred.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from data_extraction import get_raw_dataset\n",
    "\n",
    "def save_predictions(ids, predictions, output_file='./content/Result_llm.jsonl'):\n",
    "    \"\"\"Save predictions to a JSONL file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    for id_, pred in zip(ids, predictions):\n",
    "        results.append({\"id\": id_, \"label\": int(pred)})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in results:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "def save_numpy_arrays(predictions, labels=None, prefix=\"LLM\"):\n",
    "    \"\"\"Save predictions and labels as numpy arrays.\"\"\"\n",
    "    if not os.path.exists('predictions'):\n",
    "        os.makedirs('predictions')\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_file = f'predictions/{prefix}_y_test_pred.npy'\n",
    "    np.save(pred_file, np.array(predictions))\n",
    "    print(f\"Predictions saved to {pred_file}\")\n",
    "    \n",
    "    # Save labels if available\n",
    "    if labels is not None:\n",
    "        label_file = f'predictions/{prefix}_y_test.npy'\n",
    "        np.save(label_file, np.array(labels))\n",
    "        print(f\"Ground truth labels saved to {label_file}\")\n",
    "\n",
    "def zero_shot_classify(texts, device=0):\n",
    "    \"\"\"Classify texts using zero-shot classification.\"\"\"\n",
    "    # Initialize classifier\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\", \n",
    "        model=\"facebook/bart-large-mnli\",\n",
    "        device=device  # Can be set to -1 for CPU\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    for text in tqdm(texts):\n",
    "        # Truncate text to avoid token limits\n",
    "        truncated_text = text[:500]\n",
    "        \n",
    "        # Classify\n",
    "        output = classifier(\n",
    "            truncated_text,\n",
    "            candidate_labels=[\"human-written\", \"machine-generated\"],\n",
    "        )\n",
    "        \n",
    "        # Map to binary labels (0 = human, 1 = machine)\n",
    "        pred_label = 1 if output[\"labels\"][0] == \"machine-generated\" else 0\n",
    "        predictions.append(pred_label)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def evaluate_performance(predictions, labels):\n",
    "    \"\"\"Calculate and print evaluation metrics.\"\"\"\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro')\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1: {micro_f1:.4f}\")\n",
    "    \n",
    "    return accuracy, macro_f1, micro_f1\n",
    "\n",
    "def main():\n",
    "    # Check if predictions file already exists\n",
    "    result_file = './content/Result_llm.jsonl'\n",
    "    if os.path.exists(result_file):\n",
    "        print(f\"Predictions file {result_file} already exists.\")\n",
    "        # Load existing predictions\n",
    "        test_predictions = []\n",
    "        with open(result_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                test_predictions.append(data['label'])\n",
    "        print(f\"Loaded {len(test_predictions)} predictions for test set\")\n",
    "    else:\n",
    "        # Generate new predictions on test set\n",
    "        print(\"Processing test data...\")\n",
    "        X_test, ids_test = get_raw_dataset(mode='test')\n",
    "        test_predictions = zero_shot_classify(X_test)\n",
    "        save_predictions(ids_test, test_predictions, result_file)\n",
    "    \n",
    "    # Extract ground truth from test file (if available)\n",
    "    gold_file = './content/subtaskA_monolingual.jsonl'  \n",
    "    true_labels = None\n",
    "    if os.path.exists(gold_file):\n",
    "        true_labels = []\n",
    "        with open(gold_file, 'r') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                true_labels.append(item['label'])\n",
    "        print(f\"Loaded {len(true_labels)} ground truth labels\")\n",
    "        \n",
    "        # Evaluate test predictions if labels are available\n",
    "        if len(true_labels) == len(test_predictions):\n",
    "            print(\"\\nTest Set Evaluation:\")\n",
    "            evaluate_performance(test_predictions, true_labels)\n",
    "    \n",
    "    # Save test predictions and labels as numpy arrays\n",
    "    save_numpy_arrays(test_predictions, true_labels)\n",
    "    \n",
    "    # Process validation data\n",
    "    print(\"\\nProcessing validation data...\")\n",
    "    X_dev, y_dev = get_raw_dataset(mode='dev')\n",
    "    print(f\"Loaded {len(X_dev)} validation samples\")\n",
    "    \n",
    "    # Generate predictions on validation set\n",
    "    print(\"Making predictions on validation set...\")\n",
    "    dev_predictions = zero_shot_classify(X_dev, device=-1)  # Use CPU for validation\n",
    "    \n",
    "    # Evaluate validation predictions\n",
    "    print(\"\\nValidation Set Evaluation:\")\n",
    "    evaluate_performance(dev_predictions, y_dev)\n",
    "    \n",
    "    # Save validation predictions and labels\n",
    "    if not os.path.exists('predictions'):\n",
    "        os.makedirs('predictions')\n",
    "    np.save('predictions/LLM_y_dev_pred.npy', np.array(dev_predictions))\n",
    "    np.save('predictions/LLM_y_dev.npy', y_dev.to_numpy())\n",
    "    print(\"Saved validation predictions to predictions/LLM_y_dev_pred.npy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Zero-Shot Classification approach for LLM text detection\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28294e-ba89-4c7b-97b8-8acef9cfcf72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
